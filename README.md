# Apex Inference Service

Production-grade inference service demonstrating MLOps best practices:
- FastAPI based serving
- GitOps with Helm/Kustomize
- Canary deployments
- Full observability (Prometheus/OpenTelemetry)

## Development

Prerequisites:
- Python 3.11
- uv
- Docker